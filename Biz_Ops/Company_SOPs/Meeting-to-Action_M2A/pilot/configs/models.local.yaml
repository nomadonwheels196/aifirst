# Local model configuration (pilot)
llm:
  runner: ollama   # or llama.cpp
  model: gpt-oss:120b
  temperature: 0.2
  max_tokens: 2048

asr:
  runner: whisper.cpp
  model: small.en
